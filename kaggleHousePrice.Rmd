---
title: "House Price Advanced Regression_Ensemble"
author: "이춘호"
date: "2017년 10월 8일"
output: html_document
---

## House Price Advanced Regression_Ensemble 순서
1. 데이터 전처리
2. 학습을 위해 데이터 타입 변경
3. 알고리즘별 최적화 : XGBoost
4. 알고리즘별 최적화 : RandomForest
5. 알고리즘별 최적화 : SVM
6. 알고리즘별 최적화 : Keras
7. Ensemble
8. 기타 : h2o Ensemble

## 1. 데이터 전처리 
+ 데이터 준비
+ 전처리가 끝난 train 과 test 데이터를 불러온다.

```{r}
x.train <- read.csv("C:/Users/bevis/Downloads/House_Prices_Advanced_Regression_Techniques/train_eda.csv")

x.test <- read.csv("C:/Users/bevis/Downloads/House_Prices_Advanced_Regression_Techniques/test_eda.csv")

x.train <- x.train[, -1]
x.test <- x.test[, -1]
```

+ train data 파악
+ int, factor, num 형태로 존재

```{r}
str(x.train)
```

+ test data 파악
+ train data와 동일하게 int, factor, num 형태로 존재하나, $ Alley 와 같은 컬럼에서 train data에서 없었던 level("missing") 이 존재하는 경우를 파악할 수 있다. 따라서, 2개의 데이터간에 factor level을 맞추는 작업이 필요하다.

```{r}
str(x.test)
```

## 2. 학습을 위해 데이터 타입 변경
+ 학습에 사용될 알고리즘 : SVM, RandomForest
+ train, test factor level 맞춰 재생성

```{r warning=FALSE}
if (!require("dplyr")) { install.packages("dplyr") }

SalePrice <- x.train[, 82]

train_df_model <- x.train[, 1:81]
test_df <- x.test
test_id <- c(1461:2919)

train_test <- bind_rows(train_df_model, test_df) ## 합치면서 character 컬럼이 생김
ntrain <- nrow(train_df_model)

features <- names(x.train)

#convert character into factor : character 컬럼 factor 수정
for (f in features) {
    if (is.character(train_test[[f]])) {
        levels = sort(unique(train_test[[f]]))
        train_test[[f]] = as.integer(factor(train_test[[f]], levels = levels))
    }
}

#splitting whole data back again
train_x <- train_test %>% .[1:ntrain,]
test_x <- train_test %>% .[(ntrain + 1):nrow(train_test),]

train_xy <- cbind(train_x, SalePrice)
```

+ 학습에 사용될 알고리즘 : Keras(tensorflow), XGBoost
+ tensorflow를 돌리기 위해선 factor(이산형 변수)의 타입을 수정해야 한다. 방법은 2가지가 있다. 이산형 변수 속성별로 컬럼을 생성하여 "1,0" 값으로 추가하는 방식 과 이산형 변수의 관측치(level) 값으로 변경하는 방식이 있다.
+ 수정 작업은 factor level 값으로 대체하는 방식을 택하였다. 
+ train, test factor level 맞춰 재생성

```{r warning=FALSE}
## factor에서 numeric 변경 : Keras
train_test_double <- train_test

for (f in features) {
    if (is.factor(train_test_double[[f]])) {
        levels = sort(unique(train_test_double[[f]]))
        train_test_double[[f]] = as.numeric(train_test_double[[f]], levels = levels)
    }
}

#splitting whole data back again
X_train <- train_test_double %>% .[1:ntrain,]
X_test <- train_test_double %>% .[(ntrain + 1):nrow(train_test_double),]
train_XY <- cbind(X_train, SalePrice)
```

## 3. 알고리즘별 최적화 : XGBoost / LB : 0.20220
```{r}
if (!require("xgboost")) { install.packages("xgboost") }

dtrain <- xgb.DMatrix(as.matrix(X_train), label = SalePrice)
dtest <- xgb.DMatrix(as.matrix(X_test))
```

####  Hyper Parameters : Build a xgb.DMatrix object
##### 참고 - https://datascience.stackexchange.com/questions/9364/hypertuning-xgboost-parameters

+ subsample = "0.5, 0.55, 0.6, 0.65, ... , 0.95, 1.0"
+ colsample_bytree = "0.5, 0.55, 0.6, 0.65, ... , 0.95, 1.0"
+ ntrees = "100, 200, 300, ... , 800, 900, 1000"

```{r}
searchGridSubCol <- expand.grid(subsample = seq(0.5, 1, by = 0.05),
                                colsample_bytree = seq(0.5, 1, by = 0.05),
                                ntrees = seq(100,1000, by = 100))

head(searchGridSubCol)

```

##### Hyper Parameters XGBoost data frame 생성

```{r}
rmseErrorsHyperparameters_xgb <- data.frame()
```

#### Hyper Parameters 실행 및 결과 "rmseErrorsHyperparameters_xgb" 에 저장

```{r}
# 전체 실행 코드 -> for (i in 1:nrow(searchGridSubCol)) { 
for (i in 1:1) {
    #Extract Parameters to test
    currentSubsampleRate <- searchGridSubCol[i, 1]
    currentColsampleRate <- searchGridSubCol[i, 2]
    ntreesNum <- searchGridSubCol[i, 3]

    xgboostModelCV <- xgb.cv(data = dtrain, nrounds = ntreesNum, nfold = 5, showsd = TRUE,
                           metrics = "rmse", verbose = TRUE, "eval_metric" = "rmse",
                           "objective" = "reg:linear", "max.depth" = 15, "eta" = 2 / ntreesNum, "subsample" = currentSubsampleRate, "colsample_bytree" = currentColsampleRate, booster = "gbtree")

    xgboostModelCV_Score <- xgboostModelCV$evaluation_log$test_rmse_mean
    xvalidationScores <- as.data.frame(xgboostModelCV_Score)
    #Save rmse of the last iteration
    rmse <- tail(xvalidationScores$xgboostModelCV_Score, 1)

    rmseErrorsHyperparameters_xgb_1 <- data.frame(rmse, currentSubsampleRate, currentColsampleRate, ntreesNum)
    rmseErrorsHyperparameters_xgb <- rbind(rmseErrorsHyperparameters_xgb, rmseErrorsHyperparameters_xgb_1)
}

rmseErrorsHyperparameters_xgb
```

##### Hyper Parameters 중 rmse가 가장 낮은 값을 예측 모델에 적용
+ subsample : 0.95 / colsample_bytree : 0.55 / ntrees : 200 -> RMSE : 32415.44  

####  XGBoost 예측 모델 실행

```{r}
### prediction---------------------------
currentSubsampleRate <- 0.95
currentColsampleRate <- 0.55
ntreesNum <- 200

bst <- xgboost(data = dtrain, nrounds = ntreesNum, nfold = 5, showsd = TRUE,
              metrics = "rmse", verbose = TRUE, "eval_metric" = "rmse",
              "objective" = "reg:linear", "max.depth" = 15, "eta" = 2 / ntreesNum, "subsample" = currentSubsampleRate,
              "colsample_bytree" = currentColsampleRate, booster = "gbtree")

### prediction result save
pred <- data.frame(test_id,xgb_pred <- predict(bst, dtest))
colnames(pred) <- c("Id", "SalePrice")

write.csv(pred, paste0("C:/Users/bevis/Downloads/House_Prices_Advanced_Regression_Techniques/xgb_Hyper_Parameters", format(Sys.time(), "%Y%b%d_%H%M%S"), ".csv ", sep = " "), quote = F, row.names = F)

```

####  XGBoost 예측 결과 LB Score : 0.20220

## 4. 알고리즘별 최적화 : RandomForest / LB : 0.14674

```{r}
if (!require("randomForest")) { install.packages("randomForest") }
if (!require("caret")) { install.packages("caret") }
```

####  Hyper Parameters : Build a RandomForest object
##### 참고 - https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

+ mtry = "15, 16, 17, 18, ... , 28, 29, 30"
+ ntree = "1000, 1100, 1200, ... , 2300, 2400, 2500"

```{r}
seed <- 7
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "grid")

tunegrid <- expand.grid(.mtry = c(29)) # 15:30

##### function
rmseErrorsHyperparameters_rf <- list()

# ntree 100:2500
# 전체 실행 코드 -> for (ntree in seq(1000, 2500, by = 100)) {
for (ntree in 1000) {
    set.seed(seed)
    fit <- train(SalePrice ~ ., data = train_xy, method = "rf",
                 tuneGrid = tunegrid, trControl = control, ntree = ntree)
    key <- toString(ntree)
    rmseErrorsHyperparameters_rf[[key]] <- fit
}

rmseErrorsHyperparameters_rf
```

##### Hyper Parameters 중 rmse가 가장 낮은 값을 예측 모델에 적용
tail(rmseErrorsHyperparameters_rf$`1000`$results, 1) # 100~2500

+ tail(rmseErrorsHyperparameters_rf$`1200`$results,1)
+ mtry RMSE Rsquared MAE RMSESD RsquaredSD MAESD
+ 29 28876.83 0.8789629 16639.8 5809.037 0.05163747 1948.213

+ tail(rmseErrorsHyperparameters_rf$`1000`$results,1)
+ mtry RMSE Rsquared MAE RMSESD RsquaredSD MAESD
+ 29 28879.71 0.8789424 16639.33 5822.773 0.05140685 1943.522

+ tail(rmseErrorsHyperparameters_rf$`1100`$results,1)
+ mtry RMSE Rsquared MAE RMSESD RsquaredSD MAESD
+ 29 28883.11 0.8789304 16638.15 5815.917 0.05154836 

####  RandomForest 예측 모델 실행

```{r}
### prediction---------------------------
tunegrid <- expand.grid(.mtry = 29)
ntree = 1200

rf_model <- train(SalePrice ~ ., data = train_xy, method = "rf",
                 tuneGrid = tunegrid, trControl = control, ntree = ntree)

# prediction result save
pred <- data.frame(test_id, rf_pred <- predict(rf_model, test_x))
colnames(pred) <- c("Id","SalePrice")

write.csv(pred, paste0("C:/Users/bevis/Downloads/House_Prices_Advanced_Regression_Techniques/rf_Hyper_Parameters", format(Sys.time(), "%Y%b%d_%H%M%S"), ".csv ", sep = " "), quote = F, row.names = F)
```
####  RandomForest 예측 결과 LB Score : 0.14674

## 5. 알고리즘별 최적화 : Support Vector Machines / LB : 0.43200

